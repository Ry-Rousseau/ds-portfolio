genX <- function(n) {
return(
data.frame(X0 = 1,
X1 = runif(n,-5,5),
X2 = runif(n,-2,2),
X3 = runif(n,-10, 10))
)
}
# Function to generate outcome data as linear combination of IVs
genY <- function(X) {
# Add noise to the linear combination of IVs
Ylin <- 4*X$X0 + 2*X$X1 - 3*X$X2 + (1/2)*X$X3 + rnorm(nrow(X),0,0.08)
# Normalize via sigmoid function
Yp <- 1/(1+exp(-Ylin))
# Convert to binary outcome
Y <- rbinom(nrow(X),1,Yp)
return(Y)
}
# Function to get logistic yhat predictions (1 or 0)
predict_row <- function(row, coefficients) {
# row = a single row of the X matrix
# coefficients = the coefficients of the linear model
# First get values of the individual linear terms
pred_terms <- row*coefficients
# Then sum these up
yhat <- sum(pred_terms)
#Convert to probability using logit function
return(1/(1+exp(-yhat)))
}
# Function to determine mean squared error of a model
MSE <- function(ytrue, yhat) {
return(mean((ytrue-yhat)^2))
}
# Function to determine negative log likelihood of a model
NLL <- function(ytrue, yhat) {
return(-sum(log(
(yhat^ytrue)*((1-yhat)^(1-ytrue))
)))
}
# Generate the data with 2000 rows
X <- genX(2000)
y <- genY(X)
# TRAIN 1 FUNCTION IS IDENTIFIED AS TRADITIONAL STOCHASTIC GRADIENT DESCENT, DEFINED AS FOLLOWS
train <- function(X, y, l_rate, epochs) {
# X = training data
# y = true outcomes
# l_rate = learning rate
# reps = number of SGD iterations through X
# Instantiate model with basic guess of 0 for all coefficients
coefs <- rep(0, ncol(X))
# Instantiate error storage
error_store <- data.frame(epoch = 1:epochs, MSE = rep(NA, epochs), NLL = rep(NA, epochs))
# Start timer
start_time <- Sys.time()
# Iterate over epochs and random samples of x
for (b in 1:epochs) {
for (i in sample(1:nrow(X))) { # sampling the indices shuffles the order
row_vec <- as.numeric(X[i,]) # make row easier to handle
yhat_i <- predict_row(row_vec, coefficients = coefs)
# for each coefficient, apply update using partial derivative
coefs <- sapply(1:length(coefs), function (k) {
coefs[k] - l_rate*(yhat_i - y[i])*row_vec[k]
}
)
}
# calculate current error
yhat <- apply(X, 1, predict_row, coefficients = coefs)
MSE_epoch <- MSE(y, yhat)
NLL_epoch <- NLL(y, yhat)
# Store the error for this epoch
error_store[b,] <- c(b, MSE_epoch, NLL_epoch)
}
# End timer
end_time <- Sys.time()
results <- list(coefs = coefs, error_store = error_store, time_taken = end_time - start_time)
return(results) # output the final estimates
}
coef_model <- train(X = X, y = y, l_rate = 0.001, epochs = 100)
# TRAIN 2 FUNCTION IS IDENTIFIED AS STOCHASTIC GRADIENT DESCENT WITH MOMENTUM, DEFINED AS FOLLOWS
# Note: for the purposes of visualizing the algorithim, it has been modified slightly from the original
train2 <- function(X, y, l_rate, m = 0.9, epochs) {
# Initialize coefficients as 0
coefs <- rep(0, ncol(X))
# Initialize velocity as 0
v <- rep(0, ncol(X))
# Initialize error storage
error_store <- data.frame(epoch = 1:epochs, MSE = rep(NA, epochs), NLL = rep(NA, epochs))
# Start timer
start_time <- Sys.time()
for (b in 1:epochs) {
for (i in sample(1:nrow(X))) {
row_vec <- as.numeric(X[i,])
yhat_i <- predict_row(row_vec, coefficients = coefs)
for(k in 1:length(coefs)) {
v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k]
coefs[k] <- coefs[k] - v[k]
}
}
yhat <- apply(X, 1, predict_row, coefficients = coefs)
MSE_epoch <- MSE(y, yhat)
NLL_epoch <- NLL(y, yhat)
# Store the error for this epoch
error_store[b,] <- c(b, MSE_epoch, NLL_epoch)
}
# End timer
end_time <- Sys.time()
results <- list(coefs = coefs, error_store = error_store, time_taken = end_time - start_time)
return(results)
}
coef_model2 <- train2(X = X, y = y, l_rate = 0.001, epochs = 100)
# combine the two error stores together
error_store <- coef_model2$error_store %>%
select(NLL, MSE) %>%
rename(SGD2_NLL = NLL, SDG2_MSE = MSE) %>%
cbind(coef_model$error_store, .) %>%
rename(SGD1_NLL = NLL, SGD1_MSE = MSE)
# Plot the error over time
library(ggplot2)
#Negative log-likelihood
ggplot(error_store, aes(x = epoch)) +
geom_line(aes(y = SGD1_NLL, color = "red")) +
geom_line(aes(y = SGD2_NLL, color = "blue")) +
theme_minimal() +
labs(x = "Epoch", y = "Loss Function (negative log likelihood)") +
theme(legend.position = "top") +
scale_color_manual(values = c("red", "blue"), labels = c("Stochastic Gradient Descent (train1)", "Stochastic Gradient Descent with Momentum (train2)"))
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = list(coef_model$coefs, coef_model2$coefs),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Print the DataFrame
print(results_df_SGD_SGDM)
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = c(coef_model$coefs, coef_model2$coefs),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = c(coef_model$coefs, coef_model2$coefs),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
coef_model$coefs
class(coef_model$coefs)
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = c(as.character(coef_model$coefs), as.character(coef_model2$coefs)),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = list(coef_model$coefs, coef_model2$coefs),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Print the DataFrame
print(results_df_SGD_SGDM)
coef_model$coefs
coef_model2$coefs
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = c("3.1660217  1.6327201 -2.4372223  0.3872164", "3.6808136  1.8921464 -2.7876448  0.4938963"),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Print the DataFrame
print(results_df_SGD_SGDM)
# get coefficients
coef(final_mod)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(glmnet)
# EXERCISE 1
# THIS CODE GENERATES FUNCTIONS TO CREATE MOCK DATA AND PROVIDE TOOLS FOR COMPARING AND EVALUATING TRAIN1 and TRAIN2 FUNCTIONS
# Set seed for reproducibility
set.seed(89)
# Function to generate IV data - 4 IVs with 1 constant and 3 in uniform distributions
genX <- function(n) {
return(
data.frame(X0 = 1,
X1 = runif(n,-5,5),
X2 = runif(n,-2,2),
X3 = runif(n,-10, 10))
)
}
# Function to generate outcome data as linear combination of IVs
genY <- function(X) {
# Add noise to the linear combination of IVs
Ylin <- 4*X$X0 + 2*X$X1 - 3*X$X2 + (1/2)*X$X3 + rnorm(nrow(X),0,0.08)
# Normalize via sigmoid function
Yp <- 1/(1+exp(-Ylin))
# Convert to binary outcome
Y <- rbinom(nrow(X),1,Yp)
return(Y)
}
# Function to get logistic yhat predictions (1 or 0)
predict_row <- function(row, coefficients) {
# row = a single row of the X matrix
# coefficients = the coefficients of the linear model
# First get values of the individual linear terms
pred_terms <- row*coefficients
# Then sum these up
yhat <- sum(pred_terms)
#Convert to probability using logit function
return(1/(1+exp(-yhat)))
}
# Function to determine mean squared error of a model
MSE <- function(ytrue, yhat) {
return(mean((ytrue-yhat)^2))
}
# Function to determine negative log likelihood of a model
NLL <- function(ytrue, yhat) {
return(-sum(log(
(yhat^ytrue)*((1-yhat)^(1-ytrue))
)))
}
# Generate the data with 2000 rows
X <- genX(2000)
y <- genY(X)
# TRAIN 1 FUNCTION IS IDENTIFIED AS TRADITIONAL STOCHASTIC GRADIENT DESCENT, DEFINED AS FOLLOWS
train <- function(X, y, l_rate, epochs) {
# X = training data
# y = true outcomes
# l_rate = learning rate
# reps = number of SGD iterations through X
# Instantiate model with basic guess of 0 for all coefficients
coefs <- rep(0, ncol(X))
# Instantiate error storage
error_store <- data.frame(epoch = 1:epochs, MSE = rep(NA, epochs), NLL = rep(NA, epochs))
# Start timer
start_time <- Sys.time()
# Iterate over epochs and random samples of x
for (b in 1:epochs) {
for (i in sample(1:nrow(X))) { # sampling the indices shuffles the order
row_vec <- as.numeric(X[i,]) # make row easier to handle
yhat_i <- predict_row(row_vec, coefficients = coefs)
# for each coefficient, apply update using partial derivative
coefs <- sapply(1:length(coefs), function (k) {
coefs[k] - l_rate*(yhat_i - y[i])*row_vec[k]
}
)
}
# calculate current error
yhat <- apply(X, 1, predict_row, coefficients = coefs)
MSE_epoch <- MSE(y, yhat)
NLL_epoch <- NLL(y, yhat)
# Store the error for this epoch
error_store[b,] <- c(b, MSE_epoch, NLL_epoch)
}
# End timer
end_time <- Sys.time()
results <- list(coefs = coefs, error_store = error_store, time_taken = end_time - start_time)
return(results) # output the final estimates
}
coef_model <- train(X = X, y = y, l_rate = 0.001, epochs = 100)
# TRAIN 2 FUNCTION IS IDENTIFIED AS STOCHASTIC GRADIENT DESCENT WITH MOMENTUM, DEFINED AS FOLLOWS
# Note: for the purposes of visualizing the algorithim, it has been modified slightly from the original
train2 <- function(X, y, l_rate, m = 0.9, epochs) {
# Initialize coefficients as 0
coefs <- rep(0, ncol(X))
# Initialize velocity as 0
v <- rep(0, ncol(X))
# Initialize error storage
error_store <- data.frame(epoch = 1:epochs, MSE = rep(NA, epochs), NLL = rep(NA, epochs))
# Start timer
start_time <- Sys.time()
for (b in 1:epochs) {
for (i in sample(1:nrow(X))) {
row_vec <- as.numeric(X[i,])
yhat_i <- predict_row(row_vec, coefficients = coefs)
for(k in 1:length(coefs)) {
v[k] <- m*v[k] + l_rate*(yhat_i - y[i])*row_vec[k]
coefs[k] <- coefs[k] - v[k]
}
}
yhat <- apply(X, 1, predict_row, coefficients = coefs)
MSE_epoch <- MSE(y, yhat)
NLL_epoch <- NLL(y, yhat)
# Store the error for this epoch
error_store[b,] <- c(b, MSE_epoch, NLL_epoch)
}
# End timer
end_time <- Sys.time()
results <- list(coefs = coefs, error_store = error_store, time_taken = end_time - start_time)
return(results)
}
coef_model2 <- train2(X = X, y = y, l_rate = 0.001, epochs = 100)
# combine the two error stores together
error_store <- coef_model2$error_store %>%
select(NLL, MSE) %>%
rename(SGD2_NLL = NLL, SDG2_MSE = MSE) %>%
cbind(coef_model$error_store, .) %>%
rename(SGD1_NLL = NLL, SGD1_MSE = MSE)
# Plot the error over time
library(ggplot2)
#Negative log-likelihood
ggplot(error_store, aes(x = epoch)) +
geom_line(aes(y = SGD1_NLL, color = "red")) +
geom_line(aes(y = SGD2_NLL, color = "blue")) +
theme_minimal() +
labs(x = "Epoch", y = "Loss Function (negative log likelihood)") +
theme(legend.position = "top") +
scale_color_manual(values = c("red", "blue"), labels = c("Stochastic Gradient Descent (train1)", "Stochastic Gradient Descent with Momentum (train2)"))
# Neat dataframe for presenting model comparison results
results_df_SGD_SGDM <- tibble(
Method = c("Traditional Stochastic Gradient Descent", "Stochastic Gradient Descent with Momentum"),
Time_Taken = c(coef_model$time_taken, coef_model2$time_taken),
Final_Coefficients = c("3.1660217  1.6327201 -2.4372223  0.3872164", "3.6808136  1.8921464 -2.7876448  0.4938963"),
Final_NLL = c(tail(coef_model$error_store$NLL, 1), tail(coef_model2$error_store$NLL, 1))
)
# Print the DataFrame
print(results_df_SGD_SGDM)
# Define the best model function - Note this function includes many contingent functions that are defined within the function
best_model <- function(x,y) {
# FIRSTLY - DEFINE THE FUNCTIONS THAT WILL BE USED IN THE MODEL TRAINING AND VALIDATION PROCESS
#standardize the data
standardize <- function(x) {
return((x - mean(x)) / sd(x))
}
# Generate n-degree polynomial vector function
generate_poly_features <- function(x, degree) {
# x is a vector of predictor variable
# degree is the degree of the polynomial
# Generate polynomial features function
poly_features <- matrix(NA, nrow = length(x), ncol = degree)
for (i in 1:degree) {
poly_features[,i] <- x^i
}
return(poly_features)
}
# Generate ridge loss function
ridgeLoss <- function(ytrue, yhat, coefficients, lambda) {
mse <- MSE(ytrue, yhat)
l2_penalty <- lambda*sum(coefficients^2)
return(mse + l2_penalty)
}
# Generate mean squared error function
MSE <- function(ytrue, yhat) {
error <- ytrue - yhat
loss <- 1/length(y) * sum(error^2)
return(loss)
}
# DEFINE THE STOCHASTIC GRADIENT DESCENT FUNCTION WITH RIDGE REGULARIZATION
sgd_ridge_regression <- function(x, y, lambda, l_rate=0.001, epochs = 100, degree) {
# x is a vector of predictor variable
# y is a vector of true outcome variable
# lambda is the regularization parameter
# learning_rate is the step size for gradient descent
# epochs is the number of iterations
# Note: function assumes x and y are standardized, so y-intercept is estimated at 0, no need to regularize it
# Firstly, get polynomial features, with intercept at index 0
x_poly <- generate_poly_features(x, degree)
# Randomly initialize a number of coefficients equal to degree
coefs <- runif(ncol(x_poly), -0.5, 0.5)
# Loop through epochs
for(epoch in 1:epochs) {
# Loop through random samples (stochastic gradient descent)
for(i in sample(1:length(x))) {
# Get current x and y
x_var <- x[i]
y_var <- y[i]
# get matching polynomial values for current x
x_poly_i <- generate_poly_features(x_var, degree)
# Get predicted y using current coefficients and polynomial values of current x
yhat_i <- sum(coefs * x_poly_i)
# Apply update to each coefficient using partial derivative
coefs <- sapply(1:length(coefs), function (k) {
# Get gradient for current x
grad <- (yhat_i - y_var)*x_var
l2_penalty <- 2*lambda*coefs[k]
# Update coefficients
coefs[k] - l_rate*(grad + l2_penalty)
}
)
}
# Calculate yhat for all x using current coefficients
yhat <- rowSums(x_poly * coefs)
# Calculate loss for current epoch: sum of squared residuals + l2 penalty
loss_epoch <- ridgeLoss(y, yhat, coefs, lambda)
return(coefs)
}}
# DEFINE THE CROSS VALIDATION FUNCTION FOR K-FOLDS
generate_k_folds <- function(x, y, k = 5) {
#This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.
# x is a vector of predictor variable
# y is a vector of true outcome variables, same length
# k is the number of folds, 5 is default
# Number of observations
n <- length(x)
# Create a sequence of indices
indices <- sample(1:n)
# Calculate the size of each fold
fold_size <- ceiling(n / k)
# Initialize a list to store the folds
folds <- vector("list", k)
# Generate the folds
for(i in 1:k) {
# Calculate start and end indices for the fold
start_index <- ((i - 1) * fold_size) + 1
end_index <- min(i * fold_size, n)
# Get the indices for the current fold
fold_indices <- indices[start_index:end_index]
# Extract the data for the current fold
folds[[i]] <- list(
x_train = x[-fold_indices],
y_train = y[-fold_indices],
x_test = x[fold_indices],
y_test = y[fold_indices]
)
}
return(folds)
}
# Now all functions have been defined, we standardize the data to make it suitable for ridge regularization
x <- standardize(x)
y <- standardize(y)
# Next, we define a hyper parameter search method to iterate over different degrees and lambda values, trying to find a minimum of the variance and bias trade off
# The chosen search method is a semi-random grid search with discrete degree values and 30 bounded random values for the continuous lambda parameter, clustered around lambda < 1
degrees <- 1:7
lambda_values <- c(runif(10, 0, 0.1), runif(10, 0.1, 1), runif(10, 1, 10))
best_loss <- Inf
best_model <- NULL
best_degree <- NULL
best_lambda <- NULL
# Split data for cross-validation, use k = 5 as a good default for number of splits, not too large but still sufficient for cross validation
folds <- generate_k_folds(x, y, k = 5)
# perform grid search for finding optimal model hyper parameters
for(degree in degrees) {
for(lambda in lambda_values) {
# perform cross-validation and model training over k folds for each hyperparameter combination
loss_vec <- c()
for (fold in folds) {
x_train <- fold$x_train
y_train <- fold$y_train
x_val <- fold$x_test
y_val <- fold$y_test
# Train a new model for each of the 5 folds' train-test split
model_weights <- sgd_ridge_regression(x_train, y_train, lambda, degree = degree)
# Perform validation step, determine loss on validation data
x_val_poly <- generate_poly_features(x_val, degree)
yhat <- rowSums(x_val_poly * model_weights)
loss <- ridgeLoss(y_val, yhat, model_weights, lambda)
loss_vec <- c(loss_vec, loss) # store the result
}
# Compare model accuracy (minimum loss) for current hyper parameters against previous
# Final model score is taken as the average loss on the validation across the k folds
loss <- mean(loss_vec)
# if loss less than minimum previously observed, update it
if(loss < best_loss) {
best_loss <- loss
best_model <- model_weights
best_degree <- degree
best_lambda <- lambda
}
}
}
cat("Optimal Model Form as follows:\n", "Optimal polynomial degree is", best_degree, "\nOptimal lambda is", best_lambda, "\nOptimal model weights are", best_model)
cat("\nBest Validation Loss:", best_loss)
return(list(best_model, best_degree, best_lambda, best_loss))
}
# Load in the raw civil war data
load("civil_wars.RData")
# Split the data into training and testing based on 80/20 split
set.seed(123)
indices <- sample(1:nrow(civwars))
split_index <- round(nrow(civwars) * 0.8)
civwars_train <- civwars[indices[1:split_index],]
civwars_test <- civwars[indices[(split_index+1):nrow(civwars)],]
# Split the training data into predictors and outcome
civwars_trainX <- as_tibble(civwars_train) %>%
select(-c(2:9)) # Remove the irrelevant outcome variables
civwars_trainY <- civwars_train$war
# Train the model using the training data, with k = 10 folds
# 10 Folds is a common choice for k, and provides sufficient validation sets for minimizing bias and variance
# Note that cv.glmnet automatically standardizes the predictors, otherwise it would be included as a data processing step
lasso_cv <- cv.glmnet(as.matrix(civwars_trainX), civwars_trainY, alpha = 1, family = "binomial", nfolds = 10)
# Extract the optimal lambda value for the L1 penalty term
lambda_min <- lasso_cv$lambda.min
# Train final model using the optimal lambda value from cross validation
final_mod <- glmnet(as.matrix(civwars_trainX), civwars_trainY, alpha = 1, lambda = lambda_min, family = "binomial")
# get coefficients
coef(final_mod)
plot(lasso_cv$glmnet.fit, "lambda")
# Test the model using the testing data
civwars_testX <- as_tibble(civwars_test) %>%
select(-c(2:9))
civwars_testY <- civwars_test$war
# Predict the outcome using the test data
civwars_test_pred <- predict(final_mod, newx = as.matrix(civwars_testX), s = lambda_min, type = "response")
# Calculate the accuracy of the model: a standard evaluation for logistic classification equations
civwars_test_pred <- ifelse(civwars_test_pred > 0.5, 1, 0)
accuracy <- mean(civwars_test_pred == civwars_testY)
cat("Accuracy:", accuracy, "\n")
# Calculate the confusion matrix
cat("Confusion matrix:\n")
confusion_matrix <- table(civwars_testY, civwars_test_pred)
print(confusion_matrix)
# Calculate the sensitivity and specificity
sensitivity <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
specificity <- confusion_matrix[1,1] / sum(confusion_matrix[1,])
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
